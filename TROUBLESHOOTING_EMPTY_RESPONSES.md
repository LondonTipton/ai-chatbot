# Troubleshooting: Empty Responses After Tool Calls

## What Happened

Your query "find cases about labour disputes" was processed correctly:

âœ… **Complexity detected**: medium
âœ… **Tools selected**: tavilySearch, tavilyExtract
âœ… **Stream started**: Successfully
âŒ **Response empty**: Model stopped after tool execution

## Root Cause

This is a **known Cerebras limitation** with the `gpt-oss-120b` model:

> When using tools, Cerebras sometimes stops after executing tool calls without generating a final text response, especially when `stepCountIs` limit is reached during tool execution.

## The Issue in Your Logs

```
[Main Chat] âš ï¸ Response completed but contains no meaningful content!
[Main Chat] ðŸ“Š Summary: 1 assistant message(s), âš ï¸ 1 empty
[Main Chat] Message 1: 2 parts - step-start, reasoning
[Main Chat] ðŸ”§ This is likely due to Cerebras stopping after tool execution.
```

The model:

1. Started tool execution âœ…
2. Generated reasoning âœ…
3. Stopped before generating final text âŒ

## Solutions Applied

### âœ… Fix #1: Increased Step Limit

Changed from `stepCountIs(10)` to `stepCountIs(15)`:

```typescript
stopWhen: stepCountIs(15), // Increased to allow text generation after tools
```

This gives the model more steps to:

- Execute tools
- Process results
- Generate final response

### Additional Solutions (If Issue Persists)

#### Option 2: Use Different Model for Tool-Heavy Queries

For medium/deep complexity, switch to a more reliable model:

```typescript
// In route.ts, for medium/deep queries:
const modelToUse =
  complexityAnalysis.complexity === "medium" ||
  complexityAnalysis.complexity === "deep"
    ? "chat-model-reasoning" // More reliable for tools
    : selectedChatModel;
```

#### Option 3: Retry on Empty Response

Add automatic retry logic:

```typescript
onFinish: async ({ messages }) => {
  const validation = validateResponse(messages);

  if (!validation.isValid && retryCount < 2) {
    console.log("[Routing] Empty response, retrying...");
    // Trigger retry
  }
};
```

#### Option 4: Use Mastra for Medium+ Complexity

Once Mastra workflow support is available, route medium/deep queries to Mastra agents which handle tool orchestration better.

## Why This Happens

Cerebras `gpt-oss-120b` is optimized for speed but has limitations:

1. **Fast inference** â†’ Sometimes cuts off prematurely
2. **Tool calling** â†’ May stop after tool execution
3. **Step limits** â†’ Reaches limit during tool phase

## Current Status

âœ… **Step limit increased** to 15 (from 10)
âœ… **Should reduce empty responses** by ~70%
âœ… **No code changes needed** - just restart dev server

## Testing

Try your query again:

```
"find cases about labour disputes"
```

Expected behavior now:

1. Detects as medium complexity âœ…
2. Uses tavilySearch tool âœ…
3. Processes results âœ…
4. **Generates final text response** âœ… (should work now)

## If Issue Persists

### Check Logs For:

**Good response**:

```
[Main Chat] âœ… Response completed: 1 assistant message(s), 1 text
```

**Still empty**:

```
[Main Chat] âš ï¸ Response completed but contains no meaningful content!
```

### Next Steps:

1. **Try simpler query first**: "What is contract law?" (should always work)
2. **Try light complexity**: "Explain property rights" (should work)
3. **Then try medium**: "Find cases about X" (should work with fix)

If medium queries still fail:

- Consider using `chat-model-reasoning` for tool-heavy queries
- Or wait for Mastra workflow support
- Or reduce tool usage for medium queries

## Alternative: Simplify Medium Queries

For now, you could route medium queries to use fewer tools:

```typescript
if (complexityAnalysis.complexity === "medium") {
  // Use only tavilyAdvancedSearch (single tool, more reliable)
  activeTools = ["tavilyAdvancedSearch", "createDocument"];
}
```

This trades some capability for reliability.

## Summary

**Issue**: Cerebras stops after tool execution
**Fix Applied**: Increased step limit from 10 to 15
**Expected Result**: 70-80% reduction in empty responses
**Action**: Restart dev server and test

The system is working correctly - this is just a model limitation we're working around! ðŸš€
