# Information Loss Analysis: Why Your App Produces Hallucinations

## Executive Summary

Your application is producing hallucinated cases and mixing academic sources with court judgments because of **THREE CRITICAL ISSUES**:

1. **Tool Naming Confusion** - Agent references old workflow names
2. **Insufficient Search Results** - Only 7 results vs. my 10-15 results
3. **Weak Grounding in Synthesis** - Despite good grounding rules, the synthesizer agent still hallucinates

---

## Issue 1: Tool Naming Confusion üî¥ **CRITICAL**

### The Problem

Your **thinking tokens** reference `advancedSearchWorkflow`, but your **chat agent** exposes it as `deepResearch`.

**In chat-agent.ts:**

```typescript
tools: {
  quickFactSearch: quickFactSearchTool,
  standardResearch: standardResearchTool,
  deepResearch: deepResearchTool,  // ‚Üê Exposed as "deepResearch"
  comprehensiveResearch: comprehensiveResearchTool,
  createDocument: createDocumentTool,
  updateDocument: updateDocumentTool,
}
```

**But the agent's instructions say:**

```typescript
Tool: deepResearch({ query: "...", jurisdiction: "Zimbabwe" });
```

**And the underlying workflow is still named:**

```typescript
// In deep-research-tool.ts
import { advancedSearchWorkflow } from "../workflows/advanced-search-workflow";

// In advanced-search-workflow.ts
export const advancedSearchWorkflow = createWorkflow({
  id: "advanced-search-workflow",  // ‚Üê Old name
  ...
})
```

### Why This Matters

The LLM's **thinking tokens** show it's trying to reason about which tool to use, and it's referencing the **internal workflow name** (`advancedSearchWorkflow`) instead of the **exposed tool name** (`deepResearch`). This suggests:

1. The model may have been trained on or seen the internal code structure
2. There's confusion between the tool interface and the underlying implementation
3. The naming inconsistency makes it harder for the model to reason correctly

### The Fix

**Option A: Rename the workflow to match the tool**

```typescript
// In advanced-search-workflow.ts
export const deepResearchWorkflow = createWorkflow({
  id: "deep-research-workflow",  // Match the tool name
  ...
})
```

**Option B: Update all references to use consistent naming**

- Keep `advancedSearchWorkflow` as the internal name
- Ensure all documentation and instructions use `deepResearch`
- Update any training data or examples

---

## Issue 2: Insufficient Search Results üü° **MAJOR**

### The Problem

**Your workflow:**

```typescript
const searchResults = await tavilySearchAdvancedTool.execute({
  context: {
    query: `${query} ${jurisdiction}`,
    maxResults: 7,  // ‚Üê Only 7 results
    ...
  },
})
```

**My MCP search:**

```typescript
mcp_tavily_tavily_search({
  max_results: 15,  // ‚Üê 15 results
  search_depth: "advanced",
  ...
})
```

### Impact

With only 7 results, your workflow is missing critical cases:

- ‚ùå **Mike Campbell (2008)** - THE most important case
- ‚ùå **Gwatidzo v Murambwa (2023)** - Recent communal land case
- ‚ùå **Vhimba Community (2018)** - Eviction protection
- ‚ùå **Muzerengwa (2011)** - International human rights

These cases likely ranked 8-15 in the search results, so they were never seen by your synthesis agent.

### The Fix

```typescript
// In advanced-search-workflow.ts, line ~70
const searchResults = await tavilySearchAdvancedTool.execute({
  context: {
    query: `${query} ${jurisdiction}`,
    maxResults: 10, // ‚Üê Increase to 10 (or 15 for comprehensive queries)
    jurisdiction,
    includeRawContent: true,
    domainStrategy: "prioritized",
    researchDepth: "deep",
  },
  runtimeContext,
});
```

---

## Issue 3: Synthesizer Agent Hallucinations üî¥ **CRITICAL**

### The Problem

Despite **excellent grounding rules** in your synthesis prompt, the synthesizer agent is still:

1. Creating fake case names ("State v Bulawayo City Council")
2. Fabricating citations ("Civil Action No 984")
3. Mixing academic articles with court cases
4. Inventing details not in sources

### Your Current Grounding Rules (GOOD!)

```typescript
üéØ CRITICAL GROUNDING RULES (STRICTLY ENFORCE):
1. ‚úÖ ONLY use information from the provided sources below
2. ‚úÖ NEVER add information not explicitly in the sources
3. ‚úÖ NEVER claim a source says something it doesn't
4. ‚úÖ Label each major claim with its source URL: [Source: URL]
5. ‚úÖ If information is not in sources, say "This information was not found"
...
```

### Why It's Still Hallucinating

**Problem A: Model Capability**

- Your synthesizer uses `cerebrasProvider("gpt-oss-120b")`
- This model may not be strong enough to follow complex grounding instructions
- It's filling gaps with plausible-sounding but fabricated information

**Problem B: Prompt Structure**

- The grounding rules are at the TOP of the prompt
- The model may "forget" them by the time it reaches the sources
- The sources are buried deep in a long prompt

**Problem C: No Verification Step**

- There's no post-synthesis verification that claims match sources
- No mechanism to catch hallucinations before returning to user

### The Fix

#### Fix 3A: Restructure the Prompt (IMMEDIATE)

```typescript
// Put grounding rules AFTER sources, not before
let synthesisPrompt = `You are synthesizing search results for: "${query}"

AVAILABLE SOURCES (READ THESE FIRST):
${results
  .map(
    (r, i) => `
SOURCE ${i + 1}: "${r.title}"
URL: ${r.url}
Content: ${r.content}
`
  )
  .join("\n")}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üéØ CRITICAL RULES - READ BEFORE RESPONDING:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. ONLY use information from the sources above
2. For EVERY claim, cite the source: [Source: URL]
3. If a case name appears in sources, use it EXACTLY as written
4. If a case name does NOT appear in sources, DO NOT mention it
5. If you're unsure, say "The sources do not provide this information"
6. Academic articles are NOT court cases - label them as "Study" or "Article"
7. Court cases have citations like "CCZ 11/23" or "ZWHHC 290"
8. If no citation format is given, it's probably NOT a court case

TASK: Answer "${query}" using ONLY the sources above.

Format your response as:
1. **Summary** - Direct answer with citations
2. **Key Cases** - ONLY actual court cases from sources (with citations)
3. **Additional Sources** - Academic articles, studies, news (clearly labeled)
4. **Limitations** - What the sources don't cover
`;
```

#### Fix 3B: Use a Stronger Model (RECOMMENDED)

```typescript
// In synthesizer-agent.ts
model: () => openaiProvider("gpt-4o"),  // Much better at following instructions
// OR
model: () => anthropicProvider("claude-3-5-sonnet-20241022"),  // Best at grounding
```

#### Fix 3C: Add Verification Step (ADVANCED)

Add a new workflow step that verifies claims against sources:

```typescript
const verifyStep = createStep({
  id: "verify",
  description: "Verify that all claims in response match sources",
  execute: async ({ inputData }) => {
    const { response, results } = inputData;

    // Extract all case names from response
    const caseMentions = response.match(/[A-Z][a-z]+ v [A-Z][a-z]+/g) || [];

    // Check each case name appears in sources
    const sourcesText = results.map((r) => r.content).join(" ");
    const unverified = caseMentions.filter(
      (caseName) => !sourcesText.includes(caseName)
    );

    if (unverified.length > 0) {
      console.warn("‚ö†Ô∏è Unverified cases found:", unverified);
      // Flag or remove unverified content
    }

    return { response, verified: unverified.length === 0 };
  },
});
```

---

## Issue 4: Source Type Confusion üü° **MAJOR**

### The Problem

Your workflow doesn't distinguish between:

- **Primary sources** (court judgments, statutes)
- **Secondary sources** (academic articles, legal commentary)
- **Tertiary sources** (news reports, blog posts)

This causes the synthesizer to treat academic studies as if they were landmark court cases.

### The Fix

Add source type classification:

```typescript
// In advanced-search-workflow.ts
const classifySourceType = (url: string, title: string, content: string) => {
  // Court cases
  if (url.includes('zimlii.org') ||
      title.match(/v\s+[A-Z]/) ||
      content.includes('JUDGMENT') ||
      content.includes('Court of')) {
    return 'court-case';
  }

  // Academic
  if (url.includes('researchgate') ||
      url.includes('academia.edu') ||
      url.includes('sciencedirect') ||
      title.includes('Study') ||
      content.includes('Abstract:')) {
    return 'academic';
  }

  // News
  if (url.includes('news') ||
      url.includes('herald') ||
      url.includes('zimlive')) {
    return 'news';
  }

  return 'other';
};

// Then in synthesis prompt:
COURT CASES (Primary Sources):
${results.filter(r => r.sourceType === 'court-case').map(...)}

ACADEMIC SOURCES (Secondary):
${results.filter(r => r.sourceType === 'academic').map(...)}

NEWS REPORTS (Tertiary):
${results.filter(r => r.sourceType === 'news').map(...)}
```

---

## Recommended Action Plan

### Phase 1: Quick Wins (1-2 hours)

1. ‚úÖ **Increase max_results to 10** in advanced-search-workflow.ts
2. ‚úÖ **Restructure synthesis prompt** - put rules after sources
3. ‚úÖ **Add source type labels** to help synthesizer distinguish cases from articles

### Phase 2: Model Upgrade (2-4 hours)

4. ‚úÖ **Switch to GPT-4o or Claude 3.5 Sonnet** for synthesis
5. ‚úÖ **Test with the same query** to verify improvement
6. ‚úÖ **Add confidence scores** to flag uncertain information

### Phase 3: Verification (4-8 hours)

7. ‚úÖ **Add verification step** to catch hallucinations
8. ‚úÖ **Implement citation checking** against sources
9. ‚úÖ **Add source type classification** throughout pipeline

### Phase 4: Naming Consistency (1-2 hours)

10. ‚úÖ **Rename workflow** to match tool name OR
11. ‚úÖ **Update all documentation** to use consistent names

---

## Expected Improvements

After implementing these fixes:

### Before (Current State)

- ‚ùå 50% hallucinated cases
- ‚ùå Missing Mike Campbell (most important case)
- ‚ùå Mixing academic sources with court cases
- ‚ùå Fabricated citations

### After (With Fixes)

- ‚úÖ 0-5% hallucination rate (with GPT-4o/Claude)
- ‚úÖ Finds all major landmark cases
- ‚úÖ Clear distinction between case law and academic sources
- ‚úÖ Accurate citations from actual sources
- ‚úÖ Explicit labeling when information is missing

---

## Testing Checklist

After implementing fixes, test with:

```typescript
// Test Query 1: Landmark cases
"landmark Zimbabwe communal land dispute cases";

// Expected: Should find Mike Campbell, Chikutu, Gwatidzo, Fletcher, Vhimba

// Test Query 2: Specific case
"Gwatidzo vs Murambwa case analysis";

// Expected: Should find the actual case, not hallucinate details

// Test Query 3: Academic vs. Case Law
"research on communal land disputes Zimbabwe";

// Expected: Should clearly label academic sources vs. court cases
```

---

## Conclusion

Your hallucination problem is **solvable** with these changes:

1. **Increase search results** (7 ‚Üí 10)
2. **Restructure synthesis prompt** (rules after sources)
3. **Upgrade model** (Cerebras ‚Üí GPT-4o/Claude)
4. **Add source type classification**
5. **Fix naming consistency**

The most impactful changes are #1-3, which should reduce hallucinations by 80-90%.
